# Small Language Model (SLM) From Scratch

A comprehensive implementation of a Small Language Model built from scratch using PyTorch, trained on the TinyStories dataset. This project demonstrates the complete pipeline of building, training, and deploying a GPT-style transformer model with approximately 10-15 million parameters.

## üìö Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Dataset](#dataset)
- [Requirements](#requirements)
- [Installation & Setup](#installation--setup)
- [Running Locally](#running-locally)
- [Running on Google Colab](#running-on-google-colab)
- [Training Performance](#training-performance)
- [Model Configuration](#model-configuration)
- [Results](#results)
- [File Structure](#file-structure)
- [Contributing](#contributing)

## üéØ Overview

This project implements a Small Language Model (SLM) that generates creative and coherent text based on input prompts. The model is designed to be:

- **Lightweight**: ~10-15M parameters for efficient training and inference
- **Educational**: Clear, well-documented code for learning transformer architectures
- **Performant**: Optimized training with mixed precision, gradient accumulation, and advanced scheduling
- **Practical**: Ready-to-use for text generation tasks

## üèóÔ∏è Architecture

The model implements a **GPT-style transformer architecture** with the following components:

### Core Components:
- **Embedding Layers**: Token and positional embeddings
- **Transformer Blocks**: 6 layers with multi-head self-attention
- **Attention Mechanism**: Causal self-attention with optional flash attention
- **Feed-Forward Network**: 4x expansion ratio with GELU activation
- **Layer Normalization**: Pre-norm architecture for stable training
- **Weight Tying**: Shared embeddings between input and output layers

### Model Specifications:
```python
- Vocabulary Size: 50,257 (GPT-2 tokenizer)
- Context Length: 128 tokens
- Hidden Dimensions: 384
- Attention Heads: 6
- Transformer Layers: 6
- Parameters: ~10.8M
- Dropout: 0.1
- Bias: True
```

## üìñ Dataset

**TinyStories Dataset** - A synthetic dataset of short stories containing words that 3-4 year olds typically understand, generated by GPT-3.5 and GPT-4.

- **Source**: HuggingFace (`roneneldan/TinyStories`)
- **Content**: Simple, coherent short stories
- **Tokenization**: GPT-2 BPE tokenizer
- **Processing**: Stored as memory-mapped binary files for efficient training

## üìã Requirements

### Python Dependencies:
```
torch >= 1.12.0
datasets
tiktoken
numpy
matplotlib
tqdm
```

### Hardware Requirements:
- **Minimum**: 8GB RAM, any CUDA-compatible GPU
- **Recommended**: 16GB+ RAM, RTX 3080/4080 or better
- **For Colab**: T4 or A100 GPU runtime

## üöÄ Installation & Setup

### Local Installation

1. **Clone the repository:**
```bash
git clone <repository-url>
cd SLM
```

2. **Create a virtual environment:**
```bash
python -m venv slm_env
# Windows
slm_env\Scripts\activate
# Linux/Mac
source slm_env/bin/activate
```

3. **Install dependencies:**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install datasets tiktoken numpy matplotlib tqdm
```

## üíª Running Locally

1. **Open the notebook:**
```bash
jupyter notebook Small_Language_Model_Scratch.ipynb
```

2. **Execute cells sequentially:**
   - The notebook is designed to be run cell-by-cell
   - Each step builds upon the previous one
   - Training will create `train.bin` and `validation.bin` files

3. **Monitor training:**
   - Training progress is displayed with loss curves
   - Best model weights are automatically saved
   - Validation is performed every 500 iterations

## ‚òÅÔ∏è Running on Google Colab

### Quick Start:
1. **Upload the notebook** to Google Colab
2. **Set GPU runtime**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or A100)
3. **Run all cells** sequentially

### Important Notes:
- All dependencies are installed automatically via `!pip install` commands
- The notebook includes Colab-specific optimizations
- Memory management is handled automatically
- Use `runtime.unassign()` at the end to free GPU resources

### Colab-Specific Tips:
```python
# Check GPU availability
!nvidia-smi

# Monitor GPU memory
!watch -n 1 nvidia-smi

# Clear cache if needed
torch.cuda.empty_cache()
```

## ‚è±Ô∏è Training Performance

### Expected Training Times:

| GPU Type | Training Time | Notes |
|----------|---------------|-------|
| **A100** | ~30 minutes | Optimal performance with mixed precision |
| **T4** | ~8-9 hours | Slower but reliable, good for experimentation |
| **V100** | ~2-3 hours | Good balance of speed and availability |
| **RTX 3080** | ~4-5 hours | Excellent for local development |
| **RTX 4090** | ~1-2 hours | Best consumer GPU option |

### Training Configuration:
- **Iterations**: 20,000
- **Batch Size**: 32 (effective: 1,024 with gradient accumulation)
- **Learning Rate**: 1e-4 ‚Üí 5e-4 (cosine schedule with warmup)
- **Mixed Precision**: bfloat16/float16 (automatic detection)
- **Gradient Clipping**: 0.5 max norm
- **Weight Decay**: 0.1
- **Beta Values**: (0.9, 0.95)
- **Random Seed**: 42

## ‚öôÔ∏è Model Configuration

### Model Architecture:
```python
config = GPTConfig(
    vocab_size=50257,
    block_size=128,
    n_layer=6,
    n_head=6,
    n_embd=384,
    dropout=0.1,
    bias=True
)
```

### Hyperparameters:
```python
learning_rate = 1e-4
max_iters = 20000
warmup_steps = 1000
min_lr = 5e-4
eval_iters = 500
batch_size = 32
block_size = 128
gradient_accumulation_steps = 32
dropout = 0.1
```

### Optimization Configuration:
```python
# AdamW Optimizer
optimizer = torch.optim.AdamW(
    model.parameters(), 
    lr=learning_rate, 
    betas=(0.9, 0.95), 
    weight_decay=0.1, 
    eps=1e-9
)

# Learning Rate Scheduling
scheduler_warmup = LinearLR(optimizer, total_iters=warmup_steps)
scheduler_decay = CosineAnnealingLR(optimizer, T_max=max_iters - warmup_steps, eta_min=min_lr)
scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps])

# Mixed Precision Training
dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'
scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
```

## üìä Results

The trained model demonstrates:

### Text Generation Examples:
The model generates 200 tokens for each prompt:

```
Input: "Once upon a time there was a pumpkin."
Output: [Model generates 200 tokens continuing the story]

Input: "A little girl went to the woods"
Output: [Model generates 200 tokens continuing the story]
```

*Note: Actual outputs will vary based on the trained model's learned patterns from the TinyStories dataset.*

### Training Metrics:
- **Final Training Loss**: ~2.5-3.0
- **Final Validation Loss**: ~2.8-3.2
- **Convergence**: Stable after ~15,000 iterations
- **Quality**: Coherent short stories with proper grammar

## üìÅ File Structure

```
SLM/
‚îú‚îÄ‚îÄ Small_Language_Model_Scratch.ipynb  # Main notebook
‚îú‚îÄ‚îÄ README.md                           # This file
‚îú‚îÄ‚îÄ train.bin                          # Training data (generated)
‚îú‚îÄ‚îÄ validation.bin                     # Validation data (generated)
‚îú‚îÄ‚îÄ best_model_params.pt               # Saved model weights
‚îî‚îÄ‚îÄ requirements.txt                   # Dependencies (optional)
```

## üéØ What This Notebook Does

### Step 1: Dataset Import and Preparation
- Downloads TinyStories dataset from HuggingFace using `datasets` library
- Loads the complete dataset for processing

### Step 2: Data Tokenization and Storage
- Tokenizes text using GPT-2 tokenizer (`tiktoken`)
- Creates memory-mapped binary files (`train.bin` and `validation.bin`) for efficient data loading
- Processes data in batches to handle large dataset size

### Step 3: Batch Creation Function
- Implements `get_batch()` function for creating input-output pairs
- Handles memory-mapped data loading with proper device placement
- Creates sequences of length `block_size` (128 tokens)

### Step 4: Model Architecture Implementation
- Implements complete GPT-style transformer from scratch
- Includes `LayerNorm`, `CausalSelfAttention`, `MLP`, and `Block` classes
- Supports both flash attention and traditional attention mechanisms
- Implements weight tying between input and output embeddings

### Step 5: Loss Function Definition
- Implements `estimate_loss()` function for model evaluation
- Evaluates on both training and validation splits
- Uses inference mode for efficient evaluation

### Steps 6-7: Training Configuration
- Sets up training hyperparameters and device configuration
- Configures AdamW optimizer with specific beta values and weight decay
- Implements learning rate scheduling (warmup + cosine annealing)
- Sets up mixed precision training with GradScaler

### Step 8: Model Training
- Trains for 20,000 iterations with gradient accumulation
- Implements evaluation every 500 iterations
- Saves best model based on validation loss
- Includes gradient clipping and proper device management

### Step 9: Training Visualization
- Plots training and validation loss curves
- Shows model learning progress over time

### Step 10: Model Inference
- Loads the best saved model weights
- Demonstrates text generation with specific prompts
- Generates 200 tokens for each input prompt
- Includes Colab-specific cleanup with `runtime.unassign()`

## üîß Customization Options

### Modify Model Size:
```python
config = GPTConfig(
    n_layer=8,      # Increase layers
    n_head=8,       # Increase attention heads
    n_embd=512,     # Increase hidden dimensions
    block_size=256, # Increase context length
)
```

### Adjust Training:
```python
max_iters = 50000        # Train longer
batch_size = 64          # Larger batches
learning_rate = 5e-5     # Different learning rate
```

## üêõ Troubleshooting

### Common Issues:

1. **CUDA Out of Memory**:
   - Reduce `batch_size` or `block_size`
   - Increase `gradient_accumulation_steps`
   - Use `torch.cuda.empty_cache()`

2. **Slow Training**:
   - Ensure GPU is being used (`torch.cuda.is_available()`)
   - Check if mixed precision is enabled
   - Verify data loading isn't a bottleneck

3. **Poor Text Quality**:
   - Train for more iterations
   - Adjust learning rate or model size
   - Check if dataset was processed correctly

## ü§ù Contributing

Contributions are welcome! Areas for improvement:
- Add more sophisticated sampling methods (nucleus, top-k)
- Implement different model architectures (RoPE, SwiGLU)
- Add evaluation metrics (perplexity, BLEU)
- Create deployment scripts for production

## üìÑ License

This project is open source and available under the MIT License.

## üôè Acknowledgments

- **nanoGPT by Andrej Karpathy**: Some functions adapted from [nanoGPT repository](https://github.com/karpathy/nanoGPT)
- **TinyStories Dataset**: Created by Ronen Eldan and Yuanzhi Li
- **PyTorch Team**: For the excellent deep learning framework
- **HuggingFace**: For the `datasets` library and hosting the TinyStories dataset
- **Google Colab**: For providing free GPU access for research and education

---

**Happy Training! üöÄ**

For questions or issues, please open an issue in the repository or contact the maintainers.
